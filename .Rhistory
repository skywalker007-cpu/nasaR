# use columns from the datmat.
ggplot(data = data, aes(x = datmat[,1], y = datmat[,2], color = clusts)) +
geom_point(size = 2) +
scale_color_manual(values = color_list) +
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "True Group")
load("lots.Rdata")
library(ggplot2)
library(RColorBrewer)
# extract the true group and ensure it is factor.
clusts <- as.factor(clusts)
data = as.data.frame(datmat)
# make 20 distinct color from the Set3 of the color package.
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# scatter plot creaiton.
# use columns from the datmat.
ggplot(data = data, aes(x = datmat[,1], y = datmat[,2], color = clusts)) +
geom_point(size = 2) +
scale_color_manual(values = color_list) +
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "True Group") +
theme(legend.position = "right")
load("lots.Rdata")
library(ggplot2)
library(RColorBrewer)
# extract the true group and ensure it is factor.
clusts <- as.factor(clusts)
data = as.data.frame(datmat)
# make 20 distinct color from the Set3 of the color package.
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# scatter plot creaiton.
# use columns from the datmat.
ggplot(data = data, aes(x = datmat[,1], y = datmat[,2], color = clusts)) +
geom_point(size = 2) +
scale_color_manual(values = color_list) +
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "Groups")
load("lots.Rdata")
library(ggplot2)
library(RColorBrewer)
# extract the true group and ensure it is factor.
clusts <- as.factor(clusts)
data = as.data.frame(datmat)
# make 20 distinct color from the Set3 of the color package.
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# scatter plot creaiton.
# use columns from the datmat.
ggplot(data = data, aes(x = datmat[,1], y = datmat[,2], color = clusts)) +
geom_point(size = 2) +
scale_color_manual(values = color_list) +
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "Groups")
set.seed(3201)
library(mclust)
# make k-mean result for k = 20
result = kmeans(datmat, centers = 20, nstart = 10)
# compare result with pre-defined clusts for making adjused rand index
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
# repeat what we did in 9
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 10)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(3201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 10)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
library(mclust)
# load the banknote data
data(banknote)
print(head(banknote))
# remove length and status
banknote_clean = banknote[, !(names(banknote) %in% c("Status", "Length"))]
print(head(banknote_clean))
# EDA
print("Summary Table:")
summary(banknote_clean)
str(banknote_clean)
# check for missing value
print("If there are missing value:")
any(is.na(banknote_clean))
# check the unique value
print("Unique numerical value for each column:")
sapply(banknote_clean, function(x) length(unique(x)))
# scale the dataset since numeric values are not in the same scale
banknote_scaled <- as.data.frame(scale(banknote_clean))
library(GGally)
# visualizations
ggpairs(banknote_scaled)
library(ggplot2)
# hierarhical clustering
dist_matrix <- dist(banknote_scaled, method = "euclidean")
# apply different linkage methods
hc_complete <- hclust(dist_matrix, method = "complete")
hc_single <- hclust(dist_matrix, method = "single")
hc_average <- hclust(dist_matrix, method = "average")
# plot
par(mfrow = c(1,3))
# add hang attribute to make leave of the dendrogram clean
plot(hc_complete, main = "Complete Linkage", hang = -1)
plot(hc_single, main = "Single Linkage", hang = -1)
plot(hc_average, main = "Average Linkage", hang = -1)
# cutting the dendrogram first
clusters <- cutree(hc_complete, k = 2)
# make sure the cluster assignment by referencing the Status
matrix = table(clusters, banknote$Status)
print(matrix)
# lastly, make the missclassfication rate
miss_rate = 1 - sum(diag(matrix)) / sum(matrix)
cat("The Misclassification Rate is", miss_rate)
# perform the k-mean
set.seed(123)
# two cluster and make it generalized for 10 trails
kmeans_result = kmeans(banknote_scaled, centers = 2, nstart = 10)
# get results
clusters <- kmeans_result$cluster
# make table
kmean_matrix = table(clusters, banknote$Status)
print(kmean_matrix)
# miss rate
rate = 1 - sum(diag(kmean_matrix)) / sum(kmean_matrix)
cat("The missclassification rate: ", rate)
set.seed(123)
# not use the sclaed data this time
kmean_output <- kmeans(banknote_clean, centers = 2, nstart = 10)
clusters <- kmean_output$cluster
matrix_6 <- table(clusters, banknote$Status)
print(matrix_6)
rate = 1-sum(diag(matrix_6)) / sum(matrix_6)
cat("Rate =>", rate)
load("lots.Rdata")
library(ggplot2)
library(RColorBrewer)
# extract the true group and ensure it is factor.
clusts <- as.factor(clusts)
data = as.data.frame(datmat)
# make 20 distinct color from the Set3 of the color package.
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# scatter plot creaiton.
# use columns from the datmat and use clusts for the catogorical display.
ggplot(data = data, aes(x = datmat[,1], y = datmat[,2], color = clusts)) +
geom_point(size = 2) +
# use the color palette we just created
scale_color_manual(values = color_list) +
# labels
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "Groups")
set.seed(3201)
library(mclust)
# make k-mean result for k = 20
result = kmeans(datmat, centers = 20, nstart = 10)
# compare result with pre-defined clusts for making adjused rand index
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
# repeat what we did in 9
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 10)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(3201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
# read the file
cars <- read.csv("cars93.csv")
print(head(cars))
# read the file
cars <- read.csv("cars93.csv")
print(head(cars))
summary(cars)
# read the file
cars <- read.csv("cars93.csv")
print(head(cars))
str(cars)
# read the file
cars <- read.csv("cars93.csv")
str(cars)
# make character predictors like MOdel to be factor
cars$Manufacturer = as.factor(cars$Manufacturer)
cars$Model = as.factor(cars$Model)
cars$Type = as.factor(cars$Type)
str(cars)
head(cars)
# display data
head(cars)
library(dplyr)
library(ggbiplot)
library(dplyr)
install.packages("ggbiplot")
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA, labels = rownames(cars))
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA, ellipse = TRUE, var.axes = TRUE)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA, ellipse = TRUE)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA)
# extract the PC1 from rotation
PC1 <- PCA$rotation[,1]
sorted_PC1 <- sort(PC1, decreasing = TRUE)
print(sorted_PC1)
PC2 <- PCA$rotation[,2]
PC2_sort <- sort(PC2, decreasing = TRUE)
print(PC2_sort)
# Kaiser criterion (only choose the variance of PCs > 1)
pcs_var = PCA$sdev^2
pcs_var_prop = pcs_var / sum(pcs_var)
cumulative_sum_var = cumsum(pcs_var_prop)
# make a df for contain this
last_id = length(PCA)
final_df = data.frame(PC_ID = 1:last_id, Variance_explained = pcs_var_prop, Cumulative_variance = cumulative_sum_var)
print(final_df)
# Kaiser criterion (only choose the variance of PCs > 1)
pcs_var = PCA$sdev^2
pcs_var_prop = pcs_var / sum(pcs_var)
cumulative_sum_var = cumsum(pcs_var_prop)
# make a df for contain this
last_id = length(PCA$sdev)
final_df = data.frame(PC_ID = 1:last_id, Variance_explained = pcs_var_prop, Cumulative_variance = cumulative_sum_var)
print(final_df)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
str(cars_numeric)
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA)
# criterion perform
kaiser_pcs = sum(pcs_var > 1)
print(kaiser_pcs)
# criterion perform
kaiser_pcs = sum(pcs_var > 1)
cat("Number of component to keep from the Kaiser Criterion is: ", kaiser_pcs)
# criterion perform
kaiser_pcs = sum(pcs_var > 1)
cat("Number of component to keep from the Kaiser Criterion is: ", kaiser_pcs)
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
print(list_pcs_90)
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
print(list_pcs_90[0])
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
print(list_pcs_90[1])
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
# get the first element with lowest number of component
first_component <- list_pcs_90[1]
cat(first_component, "should be kept if we wish to retain at least 90%")
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
# get the first element with lowest number of component
first_component <- list_pcs_90[1]
cat(first_component, "PCs should be kept if we wish to retain at least 90%")
# scree plot
ggplot(final_df, aes(x = PC_ID, y = Variance_explained)) +
geom_point() + geom_line() +
geom_vline(xintercept = num_pcs_90, linetype = "dashed", color = "red") +  # 90% threshold marker
labs(title = "Scree Plot", x = "Principal Component", y = "Proportion of Variance Explained") +
theme_minimal()
# scree plot
ggplot(final_df, aes(x = PC_ID, y = Variance_explained)) +
geom_point() + geom_line() +
geom_vline(xintercept = first_component, linetype = "dashed", color = "red") +  # 90% threshold marker
labs(title = "Scree Plot", x = "Principal Component", y = "Proportion of Variance Explained") +
theme_minimal()
# scree plot
ggplot(final_df, aes(x = PC_ID, y = Variance_explained)) +
geom_point() + geom_line() +
# make a vertical line for threhold that retain 90%
geom_vline(xintercept = first_component, linetype = "dashed", color = "red") +  # 90% threshold marker
labs(title = "Scree Plot (Elbow)", x = "PC numbers", y = "Proportion of Variance Explained by PC")
library(mclust)
# load the banknote data
data(banknote)
print(head(banknote))
# remove length and status
banknote_clean = banknote[, !(names(banknote) %in% c("Status", "Length"))]
print(head(banknote_clean))
# EDA
print("Summary Table:")
summary(banknote_clean)
str(banknote_clean)
# check for missing value
print("If there are missing value:")
any(is.na(banknote_clean))
# check the unique value
print("Unique numerical value for each column:")
sapply(banknote_clean, function(x) length(unique(x)))
# scale the dataset since numeric values are not in the same scale
banknote_scaled <- as.data.frame(scale(banknote_clean))
library(GGally)
# visualizations
ggpairs(banknote_scaled)
library(ggplot2)
# hierarhical clustering
dist_matrix <- dist(banknote_scaled, method = "euclidean")
# apply different linkage methods
hc_complete <- hclust(dist_matrix, method = "complete")
hc_single <- hclust(dist_matrix, method = "single")
hc_average <- hclust(dist_matrix, method = "average")
# plot
par(mfrow = c(1,3))
# add hang attribute to make leave of the dendrogram clean
plot(hc_complete, main = "Complete Linkage", hang = -1)
plot(hc_single, main = "Single Linkage", hang = -1)
plot(hc_average, main = "Average Linkage", hang = -1)
# cutting the dendrogram first
clusters <- cutree(hc_complete, k = 2)
# make sure the cluster assignment by referencing the Status
matrix = table(clusters, banknote$Status)
print(matrix)
# lastly, make the missclassfication rate
miss_rate = 1 - sum(diag(matrix)) / sum(matrix)
cat("The Misclassification Rate is", miss_rate)
# perform the k-mean
set.seed(123)
# two cluster and make it generalized for 10 trails
kmeans_result = kmeans(banknote_scaled, centers = 2, nstart = 10)
# get results
clusters <- kmeans_result$cluster
# make table
kmean_matrix = table(clusters, banknote$Status)
print(kmean_matrix)
# miss rate
rate = 1 - sum(diag(kmean_matrix)) / sum(kmean_matrix)
cat("The missclassification rate: ", rate)
set.seed(123)
# not use the sclaed data this time
kmean_output <- kmeans(banknote_clean, centers = 2, nstart = 10)
clusters <- kmean_output$cluster
matrix_6 <- table(clusters, banknote$Status)
print(matrix_6)
rate = 1-sum(diag(matrix_6)) / sum(matrix_6)
cat("Rate =>", rate)
load("lots.Rdata")
library(ggplot2)
library(RColorBrewer)
# extract the true group and ensure it is factor.
clusts <- as.factor(clusts)
df = as.data.frame(datmat)
# make 20 distinct color from the Set3 of the color package.
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# scatter plot creaiton.
# use columns from the datmat and use clusts for the catogorical display.
ggplot(data = df, aes(x = df[,1], y = df[,2], color = clusts)) +
geom_point(size = 2) +
# use the color palette we just created
scale_color_manual(values = color_list) +
# labels
labs(title = "Scatterplot for Different Groups", x = "Column 1", y = "Column 2", color = "Groups")
set.seed(3201)
# make k-mean result for k = 20
result = kmeans(datmat, centers = 20, nstart = 10)
# compare result with pre-defined clusts for making adjused rand index
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
# repeat what we did in 9
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 10)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(3201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
set.seed(6201)
result = kmeans(datmat, centers = 20, nstart = 1000)
ARI <- adjustedRandIndex(result$cluster, clusts)
cat("The adjusted rand index is:", ARI)
print(mclustModelNames("VVV"))
# fit model with correspondnig G
mclust_result <- Mclust(datmat, G = c(19,20,21))
# show mclust model summary
summary(mclust_result)
# get the model name and best G
print(mclust_result$G)
print(mclust_result$modelName)
# make the similar scatter plot but with different parameter for clusters
# make a df from datmat for ggplot
mclust_df = as.data.frame(datmat)
# make new clusts from the mclust model.
clusts <- as.factor(mclust_result$classification)
# make new color pallete
color_list <- colorRampPalette(brewer.pal(9, "Set3"))(length(unique(clusts)))
# plot scatterplot
ggplot(data = mclust_df, aes(x = mclust_df[,1], y = mclust_df[,2], color = clusts)) + geom_point(size = 2) +
scale_color_manual(values = color_list) +
labs(title = "Scatterplot for 19 Groups", x = "Column 1", y = "Column 2", color = "Groups")
# read the file
cars <- read.csv("cars93.csv")
str(cars)
# make character predictors like MOdel to be factor
cars$Manufacturer = as.factor(cars$Manufacturer)
cars$Model = as.factor(cars$Model)
cars$Type = as.factor(cars$Type)
str(cars)
# display data
head(cars)
library(dplyr)
library(ggbiplot)
# filter the numeric column first
cars_numeric <- cars %>% select(where(is.numeric)) # where(is.numeric) provided by dplyr package
# make pca with sclaed version
PCA <- prcomp(cars_numeric, scale = TRUE)
# make summary
summary(PCA)
# make biplot using ggbiplot pack
ggbiplot(PCA)
# extract the PC1 from rotation
PC1 <- PCA$rotation[,1]
sorted_PC1 <- sort(PC1, decreasing = TRUE)
print(sorted_PC1)
PC2 <- PCA$rotation[,2]
PC2_sort <- sort(PC2, decreasing = TRUE)
print(PC2_sort)
# Kaiser criterion (only choose the variance of PCs > 1)
pcs_var = PCA$sdev^2
pcs_var_prop = pcs_var / sum(pcs_var)
cumulative_sum_var = cumsum(pcs_var_prop)
# sdev from PCA can get the number of component.
last_id = length(PCA$sdev)
# make a df for contain the info about variance.
final_df = data.frame(PC_ID = 1:last_id, Variance_explained = pcs_var_prop, Cumulative_variance = cumulative_sum_var)
print(final_df)
# criterion perform (eigenvalue = variance > 1)
kaiser_pcs = sum(pcs_var > 1)
cat("Number of component to keep from the Kaiser Criterion is: ", kaiser_pcs)
# at least 90% variance can be explained
list_pcs_90 <- which(cumulative_sum_var > 0.9)
# get the first element with lowest number of component
first_component <- list_pcs_90[1]
cat(first_component, "PCs should be kept if we wish to retain at least 90%")
# scree plot
ggplot(final_df, aes(x = PC_ID, y = Variance_explained)) +
geom_point() + geom_line() +
# make a vertical line for threhold that retain 90%
geom_vline(xintercept = first_component, linetype = "dashed", color = "red") +  # 90% threshold marker
labs(title = "Scree Plot (Elbow)", x = "PC numbers", y = "Proportion of Variance Explained by PC")
getwd()
setwd("F:/BroswerDownloading/UBCO_MDS_COURSES/DATA_534/nasaR")
getwd()
usethis::create_package(".")
